I'm implementing entity resolution for medical records using Python Record Linkage Toolkit. I need help finding better blocking strategies.

PROBLEM:
- Dataset: 1,228 patient records (571 unique patients), 1,121 true matching pairs
- Error rate: 42.6% of records have demographic errors (typos in names, ±1 day birthdate errors, address abbreviations, SSN transpositions)
- Current blocking strategies achieve only 76-85% recall (missing 15-24% of true matches)
- Need: ≥95% recall while keeping candidate pairs <100,000

WHAT I'VE TRIED:
1. lastname_state blocking: 76.4% recall (misses 264 matches)
2. sorted_neighborhood (window=5-7): ~80-85% recall
3. multipass (lastname+sorted+zip): ~85-90% recall
4. zip_state: 100% recall but 59,825 pairs (acceptable but could be better)

ROOT CAUSE:
High error rate means blocking keys are unreliable:
- Name typos: "Smith" vs "Smyth"
- Capitalization: "JOHNSON" vs "Johnson"
- Maiden names: "Brown" vs "Smith"
- Multiple fields may have errors simultaneously

TECHNOLOGIES:
- Python 3.12
- recordlinkage 0.16 (has block(), sortedneighbourhood(), full())
- jellyfish (Jaro-Winkler, Levenshtein, soundex, metaphone)
- scikit-learn 1.8.0

QUESTION:
What blocking strategies can achieve ≥95% recall on entity resolution with 42.6% error rate in blocking keys?

Areas to explore:
- Multi-field fuzzy blocking strategies
- Phonetic encoding (soundex/metaphone)
- Locality-sensitive hashing (LSH)
- ML-based blocking
- Canopy clustering
- Alternative paradigms (hierarchical matching, graph-based)

CONSTRAINT: Must use demographic fields only (FIRST, LAST, SSN, BIRTHDATE, ADDRESS, CITY, STATE, ZIP), no UUIDs.

Full context document available at: /Users/alex/repos/Kaggle/SyntheticMass/analysis/blocking_research_context.md
