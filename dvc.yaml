vars:
- params.yaml

stages:
  generate:
    cmd: >-
      rm -rf synthea_runner/output/synthea_raw &&
      docker build -t synthea -t synthea:$(git rev-parse --short HEAD) synthea_runner/synthea-docker/ &&
      docker run --rm
      -e JAVA_OPTS=-Xmx8g
      -v $(pwd)/synthea_runner/output/synthea_raw:/synthea/synthea/output
      synthea
      /bin/sh -c "./run_synthea -p ${generate.population} -s ${generate.seed}
      --exporter.baseDirectory=./output --exporter.csv.export=true
      --exporter.fhir.export=false Massachusetts"
    params:
    - generate.population
    - generate.seed
    deps:
    - synthea_runner/synthea-docker/Dockerfile
    outs:
    - synthea_runner/output/synthea_raw/csv:
        cache: false

  augment:
    cmd: >-
      rm -rf output/augmented &&
      docker build -t augmentation -t augmentation:$(git rev-parse --short HEAD) augmentation/ &&
      docker run --rm
      -v $(pwd)/synthea_runner/output/synthea_raw/csv:/data/input:ro
      -v $(pwd)/output/augmented:/data/output
      augmentation
      --input /data/input
      --output /data/output
      --random-seed ${augment.random_seed}
    params:
    - augment.random_seed
    deps:
    - synthea_runner/output/synthea_raw/csv
    - augmentation/Dockerfile
    - augmentation/requirements-runtime.txt
    - augmentation/cli/augment.py
    - augmentation/config/default_config.yaml
    outs:
    - output/augmented:
        cache: false

  resolve:
    cmd: >-
      rm -rf output/resolved &&
      docker build -f entity_resolution/Dockerfile -t entity_resolution -t entity_resolution:$(git rev-parse --short HEAD) . &&
      docker run --rm
      -v $(pwd)/output/augmented:/data/augmented:ro
      -v $(pwd)/output/resolved:/data/resolved
      entity_resolution
      python -m entity_resolution.resolve
      --augmented-dir /data/augmented
      --output-dir /data/resolved
      --config entity_resolution/config/matching_config.yaml
    params:
    - resolve.auto_match_probability
    - resolve.auto_reject_probability
    deps:
    - output/augmented
    - entity_resolution/Dockerfile
    - entity_resolution/resolve.py
    - entity_resolution/core/
    - entity_resolution/config/matching_config.yaml
    - entity_resolution/requirements-runtime.txt
    - shared/
    outs:
    - output/resolved:
        cache: false

  infer:
    cmd: >-
      rm -rf output/inferred &&
      mkdir -p output/inferred &&
      llm_classifier/infer_remote.sh
      output/resolved/gray_zone_pairs.parquet
      output/inferred/predictions.parquet
    deps:
    - llm_classifier/Dockerfile
    - llm_classifier/_remote_helpers.sh
    - llm_classifier/infer_remote.sh
    - llm_classifier/infer_classifier.py
    - llm_classifier/requirements.txt
    - llm_classifier/launch_pod.sh
    - output/resolved/gray_zone_pairs.parquet
    outs:
    - output/inferred/predictions.parquet:
        cache: false

  golden_records:
    cmd: >-
      rm -rf output/golden_records &&
      docker build -f entity_resolution/Dockerfile -t entity_resolution -t entity_resolution:$(git rev-parse --short HEAD) . &&
      docker run --rm
      -v $(pwd)/output/augmented:/data/augmented:ro
      -v $(pwd)/output/resolved:/data/resolved:ro
      -v $(pwd)/output/inferred:/data/inferred:ro
      -v $(pwd)/output/golden_records:/data/golden_records
      entity_resolution
      python -m entity_resolution.build_golden_records
      --augmented-dir /data/augmented
      --auto-matches /data/resolved/auto_matches.parquet
      --predictions /data/inferred/predictions.parquet
      --features /data/resolved/features.parquet
      --output-dir /data/golden_records
      --config entity_resolution/config/matching_config.yaml
    deps:
    - output/resolved/auto_matches.parquet
    - output/resolved/features.parquet
    - output/inferred/predictions.parquet
    - output/augmented
    - entity_resolution/build_golden_records.py
    - entity_resolution/core/
    - shared/
    outs:
    - output/golden_records:
        cache: false

  generate_training:
    cmd: >-
      rm -rf output/training/synthea_raw &&
      docker build -t synthea -t synthea:$(git rev-parse --short HEAD) synthea_runner/synthea-docker/ &&
      docker run --rm
      -e JAVA_OPTS=-Xmx8g
      -v $(pwd)/output/training/synthea_raw:/synthea/synthea/output
      synthea
      /bin/sh -c "./run_synthea -p ${generate_training.population} -s ${generate_training.seed}
      --exporter.baseDirectory=./output --exporter.csv.export=true
      --exporter.fhir.export=false Massachusetts"
    params:
    - generate_training.population
    - generate_training.seed
    deps:
    - synthea_runner/synthea-docker/Dockerfile
    outs:
    - output/training/synthea_raw/csv:
        cache: false

  augment_training:
    cmd: >-
      rm -rf output/training/augmented &&
      docker build -t augmentation -t augmentation:$(git rev-parse --short HEAD) augmentation/ &&
      docker run --rm
      -v $(pwd)/output/training/synthea_raw/csv:/data/input:ro
      -v $(pwd)/output/training/augmented:/data/output
      augmentation
      --input /data/input
      --output /data/output
      --random-seed ${augment_training.random_seed}
    params:
    - augment_training.random_seed
    deps:
    - output/training/synthea_raw/csv
    - augmentation/Dockerfile
    - augmentation/requirements-runtime.txt
    - augmentation/cli/augment.py
    - augmentation/config/default_config.yaml
    outs:
    - output/training/augmented:
        cache: false

  prepare_dataset:
    cmd: >-
      rm -rf output/training/dataset &&
      mkdir -p output/training/dataset &&
      docker build -f llm_classifier/Dockerfile.prepare -t prepare-dataset -t prepare-dataset:$(git rev-parse --short HEAD) . &&
      docker run --rm
      -v $(pwd)/output/training/augmented:/data/augmented:ro
      -v $(pwd)/output/training/dataset:/data/dataset
      -v $(pwd)/llm_classifier/.env:/app/.env:ro
      prepare-dataset
      python llm_classifier/prepare_dataset.py
      --augmented-dir /data/augmented
      --output-dir /data/dataset
      --seed ${prepare_dataset.seed}
    params:
    - prepare_dataset.seed
    deps:
    - output/training/augmented
    - llm_classifier/prepare_dataset.py
    - llm_classifier/Dockerfile.prepare
    - llm_classifier/requirements-prepare.txt
    - shared/summarize.py
    - shared/data_loader.py
    - shared/ground_truth.py
    - shared/medical_records.py
    outs:
    - output/training/dataset/dataset_info.json:
        cache: false

  train:
    cmd: >-
      rm -rf output/training/train &&
      mkdir -p output/training/train &&
      llm_classifier/train_remote.sh
      --gpu-type "NVIDIA H100 80GB HBM3"
      output/training/train
      --
      --epochs ${train.epochs}
      --batch-size ${train.batch_size}
      --lr ${train.lr}
      --max-length ${train.max_length}
      --lora-r ${train.lora_r}
    params:
    - train.epochs
    - train.batch_size
    - train.lr
    - train.max_length
    - train.lora_r
    deps:
    - output/training/dataset/dataset_info.json
    - llm_classifier/train_classifier.py
    - llm_classifier/train_remote.sh
    - llm_classifier/_remote_helpers.sh
    - llm_classifier/launch_pod.sh
    - llm_classifier/Dockerfile
    - llm_classifier/requirements.txt
    outs:
    - output/training/train/train_metrics.json:
        cache: false

  export:
    cmd: >-
      rm -rf output/training/export &&
      mkdir -p output/training/export &&
      llm_classifier/export_remote.sh
      --gpu-type "NVIDIA H100 80GB HBM3"
      output/training/export
    deps:
    - output/training/train/train_metrics.json
    - llm_classifier/export_model.py
    - llm_classifier/export_remote.sh
    - llm_classifier/_remote_helpers.sh
    - llm_classifier/launch_pod.sh
    - llm_classifier/Dockerfile
    - llm_classifier/requirements.txt
    outs:
    - output/training/export/export_info.json:
        cache: false
