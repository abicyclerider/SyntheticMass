{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Claude Opus on Sparse Pairs\n",
    "\n",
    "The MedGemma 27B benchmark on the new 6,482-pair test set showed F1=0.634,\n",
    "with performance strongly correlated to prompt length. Sparse pairs (<2,000 chars)\n",
    "had F1=0.278. This notebook benchmarks Claude Opus on 100 sparse pairs to see\n",
    "if the difficulty is inherent or a model limitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: claude-opus-4-6 (via claude-agent-sdk, no API key needed)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "MODEL = \"claude-opus-4-6\"\n",
    "print(f\"Using model: {MODEL} (via claude-agent-sdk, no API key needed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and select sparse pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: 6482 pairs\n",
      "Sparse pairs (<2000 chars): 915\n",
      "  Match: 695, Non-match: 220\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"abicyclerider/entity-resolution-pairs\")\n",
    "test = ds[\"test\"]\n",
    "print(f\"Test set: {len(test)} pairs\")\n",
    "\n",
    "# Identify sparse pairs (< 2000 chars)\n",
    "sparse_indices = [\n",
    "    i for i in range(len(test))\n",
    "    if len(test[i][\"messages\"][0][\"content\"]) < 2000\n",
    "]\n",
    "print(f\"Sparse pairs (<2000 chars): {len(sparse_indices)}\")\n",
    "\n",
    "# Label balance in sparse pairs\n",
    "sparse_labels = [test[i][\"messages\"][1][\"content\"] == \"True\" for i in sparse_indices]\n",
    "print(f\"  Match: {sum(sparse_labels)}, Non-match: {len(sparse_labels) - sum(sparse_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 100 sparse pairs (seed=42)\n",
      "  Match: 50, Non-match: 50\n",
      "  Prompt length: min=408, median=1513, max=1999\n"
     ]
    }
   ],
   "source": [
    "# Sample 100 sparse pairs (balanced)\n",
    "SEED = 42\n",
    "N_SAMPLE = 100\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "match_idx = [i for i in sparse_indices if test[i][\"messages\"][1][\"content\"] == \"True\"]\n",
    "non_match_idx = [i for i in sparse_indices if test[i][\"messages\"][1][\"content\"] == \"False\"]\n",
    "\n",
    "n_each = N_SAMPLE // 2\n",
    "sampled = random.sample(match_idx, n_each) + random.sample(non_match_idx, n_each)\n",
    "random.shuffle(sampled)\n",
    "\n",
    "prompts = [test[i][\"messages\"][0][\"content\"] for i in sampled]\n",
    "labels = [test[i][\"messages\"][1][\"content\"] == \"True\" for i in sampled]\n",
    "\n",
    "lengths = [len(p) for p in prompts]\n",
    "print(f\"Sampled {len(sampled)} sparse pairs (seed={SEED})\")\n",
    "print(f\"  Match: {sum(labels)}, Non-match: {len(labels) - sum(labels)}\")\n",
    "print(f\"  Prompt length: min={min(lengths)}, median={sorted(lengths)[len(lengths)//2]}, max={max(lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Claude Opus inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are a medical entity resolution assistant. \"\n",
    "    \"Compare the two patient records and respond with only \"\n",
    "    \"True (same patient) or False (different patients).\"\n",
    ")\n",
    "\n",
    "\n",
    "def parse_prediction(text):\n",
    "    text = text.strip().lower()\n",
    "    if \"true\" in text:\n",
    "        return True\n",
    "    elif \"false\" in text:\n",
    "        return False\n",
    "    return None\n",
    "\n",
    "\n",
    "async def call_claude_async(prompt, system=SYSTEM_PROMPT):\n",
    "    \"\"\"Call Claude via claude-agent-sdk (subscription auth, no API key).\"\"\"\n",
    "    from claude_agent_sdk import query, ClaudeAgentOptions\n",
    "\n",
    "    result_parts = []\n",
    "    async for message in query(\n",
    "        prompt=prompt,\n",
    "        options=ClaudeAgentOptions(\n",
    "            model=MODEL,\n",
    "            max_turns=1,\n",
    "            allowed_tools=[],\n",
    "            system_prompt=system,\n",
    "        ),\n",
    "    ):\n",
    "        if hasattr(message, \"content\"):\n",
    "            if isinstance(message.content, list):\n",
    "                for block in message.content:\n",
    "                    if hasattr(block, \"text\"):\n",
    "                        result_parts.append(block.text)\n",
    "            elif isinstance(message.content, str):\n",
    "                result_parts.append(message.content)\n",
    "\n",
    "    return \"\\n\".join(result_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test response: 'False'\n",
      "Parsed: False\n",
      "True label: False\n"
     ]
    }
   ],
   "source": [
    "# Quick test (1 pair) — uses subscription auth via claude-agent-sdk\n",
    "test_resp = await call_claude_async(prompts[0])\n",
    "print(f\"Test response: '{test_resp}'\")\n",
    "print(f\"Parsed: {parse_prediction(test_resp)}\")\n",
    "print(f\"True label: {labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [25/100] acc=0.880, unparseable=0, elapsed=62s (2.5s/pair)\n",
      "  [50/100] acc=0.860, unparseable=0, elapsed=132s (2.6s/pair)\n",
      "  [75/100] acc=0.813, unparseable=0, elapsed=198s (2.6s/pair)\n",
      "  [100/100] acc=0.800, unparseable=0, elapsed=270s (2.7s/pair)\n",
      "\n",
      "Done in 270s (2.7s/pair)\n"
     ]
    }
   ],
   "source": [
    "# Run all 100 pairs via claude-agent-sdk\n",
    "predictions = []\n",
    "raw_responses = []\n",
    "pair_times = []\n",
    "unparseable = []\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for i, (prompt, label) in enumerate(zip(prompts, labels)):\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        raw = await call_claude_async(prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR on pair {i}: {e}\")\n",
    "        raw = \"\"\n",
    "    elapsed = time.time() - t0\n",
    "    pair_times.append(elapsed)\n",
    "    raw_responses.append(raw)\n",
    "\n",
    "    pred = parse_prediction(raw)\n",
    "    predictions.append(pred)\n",
    "    if pred is None:\n",
    "        unparseable.append(i)\n",
    "\n",
    "    if (i + 1) % 25 == 0:\n",
    "        valid = [p for p in predictions if p is not None]\n",
    "        valid_l = [l for p, l in zip(predictions, labels) if p is not None]\n",
    "        acc = accuracy_score(valid_l, valid) if valid else 0\n",
    "        total = time.time() - t_start\n",
    "        print(\n",
    "            f\"  [{i+1}/{len(prompts)}] acc={acc:.3f}, \"\n",
    "            f\"unparseable={len(unparseable)}, \"\n",
    "            f\"elapsed={total:.0f}s ({total/(i+1):.1f}s/pair)\"\n",
    "        )\n",
    "\n",
    "total_time = time.time() - t_start\n",
    "print(f\"\\nDone in {total_time:.0f}s ({total_time/len(prompts):.1f}s/pair)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Claude Opus — Sparse Pairs (100 pairs, <2000 chars)\n",
      "============================================================\n",
      "Parseable: 100/100 (0 unparseable)\n",
      "    accuracy: 0.800\n",
      "   precision: 0.841\n",
      "      recall: 0.740\n",
      "          f1: 0.787\n",
      "\n",
      "Confusion matrix:\n",
      "  TN=43  FP=7\n",
      "  FN=13  TP=37\n",
      "\n",
      "Timing: 270s total (2.7s/pair)\n"
     ]
    }
   ],
   "source": [
    "valid_preds = [p for p in predictions if p is not None]\n",
    "valid_labels = [l for p, l in zip(predictions, labels) if p is not None]\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\": round(accuracy_score(valid_labels, valid_preds), 4),\n",
    "    \"precision\": round(precision_score(valid_labels, valid_preds, zero_division=0), 4),\n",
    "    \"recall\": round(recall_score(valid_labels, valid_preds, zero_division=0), 4),\n",
    "    \"f1\": round(f1_score(valid_labels, valid_preds, zero_division=0), 4),\n",
    "}\n",
    "cm = confusion_matrix(valid_labels, valid_preds).tolist()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Claude Opus — Sparse Pairs ({len(sampled)} pairs, <2000 chars)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Parseable: {len(valid_preds)}/{len(predictions)} ({len(unparseable)} unparseable)\")\n",
    "for m, v in metrics.items():\n",
    "    print(f\"  {m:>10s}: {v:.3f}\")\n",
    "print(f\"\\nConfusion matrix:\")\n",
    "print(f\"  TN={cm[0][0]}  FP={cm[0][1]}\")\n",
    "print(f\"  FN={cm[1][0]}  TP={cm[1][1]}\")\n",
    "print(f\"\\nTiming: {total_time:.0f}s total ({total_time/len(prompts):.1f}s/pair)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Comparison\n",
      "============================================================\n",
      "Model                              Acc    Prec     Rec      F1  Dataset\n",
      "------------------------------------------------------------------------------------------\n",
      "MedGemma 27B (all 6482)          0.729   0.977   0.469   0.634  new test set\n",
      "MedGemma 27B (sparse only)       0.358   0.958   0.163   0.278  <2000 chars, n=915\n",
      "MedGemma 27B (old 338)           0.799   0.848   0.728   0.783  original test set\n",
      "Claude Opus (old 338)            0.940   0.919   0.953   0.936  original test set\n",
      "Claude Opus (THIS RUN)           0.800   0.841   0.740   0.787  sparse, 100 pairs\n"
     ]
    }
   ],
   "source": [
    "# Comparison table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "baselines = [\n",
    "    (\"MedGemma 27B (all 6482)\",    0.729, 0.977, 0.469, 0.634, \"new test set\"),\n",
    "    (\"MedGemma 27B (sparse only)\", 0.358, 0.958, 0.163, 0.278, \"<2000 chars, n=915\"),\n",
    "    (\"MedGemma 27B (old 338)\",     0.799, 0.848, 0.728, 0.783, \"original test set\"),\n",
    "    (\"Claude Opus (old 338)\",      0.940, 0.919, 0.953, 0.936, \"original test set\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Model':<30s}  {'Acc':>6s}  {'Prec':>6s}  {'Rec':>6s}  {'F1':>6s}  Dataset\")\n",
    "print(\"-\" * 90)\n",
    "for name, acc, prec, rec, f1, note in baselines:\n",
    "    print(f\"{name:<30s}  {acc:>6.3f}  {prec:>6.3f}  {rec:>6.3f}  {f1:>6.3f}  {note}\")\n",
    "print(f\"{'Claude Opus (THIS RUN)':<30s}  {metrics['accuracy']:>6.3f}  \"\n",
    "      f\"{metrics['precision']:>6.3f}  {metrics['recall']:>6.3f}  {metrics['f1']:>6.3f}  \"\n",
    "      f\"sparse, {len(sampled)} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to benchmark_claude_sparse_results.json\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "results = {\n",
    "    \"model\": MODEL,\n",
    "    \"system_prompt\": SYSTEM_PROMPT,\n",
    "    \"dataset\": \"abicyclerider/entity-resolution-pairs\",\n",
    "    \"subset\": \"sparse (<2000 chars)\",\n",
    "    \"n_sampled\": len(sampled),\n",
    "    \"n_parseable\": len(valid_preds),\n",
    "    \"seed\": SEED,\n",
    "    \"metrics\": metrics,\n",
    "    \"confusion_matrix\": cm,\n",
    "    \"timing_s\": round(total_time, 1),\n",
    "    \"per_pair\": [\n",
    "        {\n",
    "            \"dataset_index\": sampled[i],\n",
    "            \"true_label\": labels[i],\n",
    "            \"prediction\": predictions[i],\n",
    "            \"raw_response\": raw_responses[i],\n",
    "            \"prompt_length\": len(prompts[i]),\n",
    "            \"time_s\": round(pair_times[i], 2),\n",
    "        }\n",
    "        for i in range(len(sampled))\n",
    "    ],\n",
    "}\n",
    "\n",
    "out_path = \"benchmark_claude_sparse_results.json\"\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Results saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total errors: 20/100\n",
      "  False negatives (missed matches): 13\n",
      "  False positives (false alarms):   7\n",
      "\n",
      "============================================================\n",
      "Sample false negatives\n",
      "============================================================\n",
      "\n",
      "--- FN 1 (dataset idx 1256, 1064 chars) ---\n",
      "You are a medical record matching expert. Compare these two patient medical records and determine if they belong to the same patient based only on their clinical history.\n",
      "\n",
      "Record A:\n",
      "CONDITIONS:\n",
      "  2020: Medication review due (situation)\n",
      "  2021: Sprain (morphologic abnormality); Sprain of ankle\n",
      "\n",
      "MEDICATIONS:\n",
      "- Naproxen sodium 220 MG Oral Tablet (2021–2021)\n",
      "\n",
      "OBSERVATIONS:\n",
      "- Body Height: 174.1 cm (2020-08-26)\n",
      "- Body Weight: 83.2 kg (2020-08-26)\n",
      "- Body Mass Index: 27.5 kg/m2 (2020-08-26)\n",
      "- Systolic Blood Pressure: 125.0 mm[Hg] (2020-08-26)\n",
      "- Diastolic Blood Pressure: 80.0 mm[Hg] (2020-08-26)\n",
      "\n",
      "PROCE\n",
      "...\n",
      "  Response: 'False' | True: True\n",
      "\n",
      "--- FN 2 (dataset idx 1560, 1824 chars) ---\n",
      "You are a medical record matching expert. Compare these two patient medical records and determine if they belong to the same patient based only on their clinical history.\n",
      "\n",
      "Record A:\n",
      "CONDITIONS:\n",
      "  2024: Normal pregnancy\n",
      "  2025: Normal pregnancy\n",
      "\n",
      "PROCEDURES:\n",
      "- Standard pregnancy test (2024, 2025)\n",
      "- Ultrasound scan for fetal viability (2024, 2025)\n",
      "- Evaluation of uterine fundal height (2025)\n",
      "- Auscultation of the fetal heart (2025)\n",
      "- Blood group typing (procedure) (2025)\n",
      "- Hemogram  automated  with RBC  WBC  Hgb  Hct  Indices  Platelet count  and manual WBC differential (2025)\n",
      "- Hepatitis B Surfa\n",
      "...\n",
      "  Response: 'False' | True: True\n",
      "\n",
      "--- FN 3 (dataset idx 3366, 1283 chars) ---\n",
      "You are a medical record matching expert. Compare these two patient medical records and determine if they belong to the same patient based only on their clinical history.\n",
      "\n",
      "Record A:\n",
      "CONDITIONS:\n",
      "  2010: Medication review due (situation); Medication review due (situation)\n",
      "  2011: Medication review due (situation); Medication review due (situation); Medication review due (situation); Medication review due (situation)\n",
      "  2012: Medication review due (situation)\n",
      "  2013: Medication review due (situation)\n",
      "  2014: Medication review due (situation)\n",
      "  2017: Sprain (morphologic abnormality); Sprain of ankl\n",
      "...\n",
      "  Response: 'False' | True: True\n"
     ]
    }
   ],
   "source": [
    "# Show misclassified pairs\n",
    "errors = [\n",
    "    (i, sampled[i], labels[i], predictions[i], raw_responses[i], len(prompts[i]))\n",
    "    for i in range(len(sampled))\n",
    "    if predictions[i] is not None and predictions[i] != labels[i]\n",
    "]\n",
    "\n",
    "print(f\"Total errors: {len(errors)}/{len(valid_preds)}\")\n",
    "fn = [e for e in errors if e[2] == True and e[3] == False]\n",
    "fp = [e for e in errors if e[2] == False and e[3] == True]\n",
    "print(f\"  False negatives (missed matches): {len(fn)}\")\n",
    "print(f\"  False positives (false alarms):   {len(fp)}\")\n",
    "\n",
    "# Show first 3 FN examples\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Sample false negatives\")\n",
    "print(\"=\" * 60)\n",
    "for idx, (i, ds_idx, true_l, pred, resp, length) in enumerate(fn[:3]):\n",
    "    print(f\"\\n--- FN {idx+1} (dataset idx {ds_idx}, {length} chars) ---\")\n",
    "    print(prompts[i][:600])\n",
    "    if len(prompts[i]) > 600:\n",
    "        print(\"...\")\n",
    "    print(f\"  Response: '{resp}' | True: {true_l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
