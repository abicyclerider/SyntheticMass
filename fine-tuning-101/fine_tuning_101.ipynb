{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning 101: LoRA on Apple Silicon\n",
    "\n",
    "This notebook walks through fine-tuning **Google Gemma 3 1B** using **LoRA** (Low-Rank Adaptation) on a MacBook Pro with Apple Silicon. By the end, you'll have a model that's been fine-tuned on medical Q&A data, and you'll understand each step well enough to scale up to larger models on GPU.\n",
    "\n",
    "**What we're doing:**\n",
    "- Loading a 1B parameter model (~4GB)\n",
    "- Adding tiny trainable LoRA adapters (~3.4M params, ~0.3% of the model)\n",
    "- Training on 1,000 medical flashcard Q&A pairs\n",
    "- Comparing before/after responses\n",
    "\n",
    "**Time:** ~20 minutes end-to-end on M3 Pro (18GB RAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Check\n",
    "\n",
    "First, let's verify that MPS (Metal Performance Shaders) is available — this is Apple Silicon's GPU acceleration for PyTorch. We also set an environment variable so that any operations not yet supported on MPS will silently fall back to CPU instead of crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0\n",
      "MPS available: True\n",
      "MPS built: True\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# MPS doesn't support every PyTorch op yet. This flag makes unsupported ops\n",
    "# fall back to CPU automatically instead of raising an error.\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# If you hit OOM during training, uncomment this line. It lets MPS allocate\n",
    "# beyond its default memory limit (at the cost of potential system slowdown).\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HuggingFace Login\n",
    "\n",
    "Gemma is a **gated model** — you need to:\n",
    "1. Have a HuggingFace account\n",
    "2. Accept Google's terms at https://huggingface.co/google/gemma-3-1b-it\n",
    "3. Create an access token at https://huggingface.co/settings/tokens\n",
    "\n",
    "Create a `.env` file in this directory with your token:\n",
    "```\n",
    "HF_TOKEN=hf_your_token_here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as: abicyclerider\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login, whoami\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "token = os.environ.get(\"HF_TOKEN\")\n",
    "if not token:\n",
    "    raise ValueError(\"HF_TOKEN not found. Create a .env file with: HF_TOKEN=hf_your_token_here\")\n",
    "\n",
    "login(token=token)\n",
    "info = whoami()\n",
    "print(f\"Logged in as: {info['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Base Model\n",
    "\n",
    "We're using **Gemma 3 1B Instruct** (`google/gemma-3-1b-it`), the smallest model in Google's Gemma family. It shares the same architecture as MedGemma, so what you learn here transfers directly.\n",
    "\n",
    "**Why float32?** Two Apple Silicon constraints force our hand:\n",
    "- **bfloat16** is not supported on MPS\n",
    "- **float16** causes numerical issues with Gemma 3's architecture\n",
    "\n",
    "So we use float32, which means ~4GB for the model weights. With LoRA overhead and training state, expect ~10-13GB total memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from google/gemma-3-1b-it...\n",
      "Loading model from google/gemma-3-1b-it in float32...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ab2d5a4f57447c8303451c6a4fc2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded on: mps:0\n",
      "Total parameters: 999,885,952\n",
      "Model dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"google/gemma-3-1b-it\"\n",
    "\n",
    "print(f\"Loading tokenizer from {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "print(f\"Loading model from {MODEL_ID} in float32...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    dtype=torch.float32,\n",
    "    device_map=\"mps\",\n",
    ")\n",
    "\n",
    "print(f\"\\nModel loaded on: {model.device}\")\n",
    "print(f\"Total parameters: {model.num_parameters():,}\")\n",
    "print(f\"Model dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peek at the model architecture. The key thing to notice is the repeated `GemmaDecoderLayer` blocks — each one contains attention layers (`q_proj`, `k_proj`, `v_proj`, `o_proj`) and feed-forward layers. LoRA will target the attention projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma3DecoderLayer(\n",
      "  (self_attn): Gemma3Attention(\n",
      "    (q_proj): Linear(in_features=1152, out_features=1024, bias=False)\n",
      "    (k_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=1152, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=1024, out_features=1152, bias=False)\n",
      "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Gemma3MLP(\n",
      "    (gate_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
      "    (up_proj): Linear(in_features=1152, out_features=6912, bias=False)\n",
      "    (down_proj): Linear(in_features=6912, out_features=1152, bias=False)\n",
      "    (act_fn): GELUTanh()\n",
      "  )\n",
      "  (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "  (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "  (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "  (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Show just the first decoder layer to see the structure\n",
    "print(model.model.layers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Base Model\n",
    "\n",
    "Before fine-tuning, let's see how the base model responds to medical questions. We'll save these responses and compare them to the fine-tuned model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASE MODEL RESPONSES\n",
      "============================================================\n",
      "\n",
      "Q: What are the main symptoms of Type 2 diabetes?\n",
      "A: Okay, let's break down the main symptoms of Type 2 diabetes. It's really important to remember that many people with Type 2 diabetes have *no* noticeable symptoms at all, especially in the early stages. This is why regular check-ups are so crucial. However, when symptoms do appear, they can be varied and often mimic other conditions.\n",
      "\n",
      "Here's a breakdown of the common symptoms, grouped by severity:\n",
      "\n",
      "**1. Early & Subtle Symptoms (Often Overlooked):**\n",
      "\n",
      "* **Increased Thirst (Polydipsia):** Feeling v\n",
      "----------------------------------------\n",
      "\n",
      "Q: What is the mechanism of action of metformin?\n",
      "A: Metformin is a cornerstone medication for type 2 diabetes, and its mechanism of action is complex and still being actively researched. It’s not a single \"magic bullet,\" but rather a multifaceted effect on glucose metabolism. Here's a breakdown of the current understanding:\n",
      "\n",
      "**1. Primarily Reduces Glucose Production in the Liver:**\n",
      "\n",
      "* **AMP-Activated Protein Kinase (AMPK) Activation:** This is the most well-established mechanism. Metformin primarily works by activating AMPK, a cellular energy sen\n",
      "----------------------------------------\n",
      "\n",
      "Q: What are the risk factors for developing hypertension?\n",
      "A: Okay, let's break down the risk factors for developing hypertension (high blood pressure). It’s a complex issue with many contributing factors, and often it's a combination of several things. Here's a breakdown, categorized for clarity:\n",
      "\n",
      "**1. Lifestyle Factors (These are often the biggest contributors):**\n",
      "\n",
      "* **Diet:**\n",
      "    * **High sodium intake:** This is arguably the *most* significant risk factor.  Excess sodium causes your body to retain water, increasing blood volume and raising blood pressu\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_questions = [\n",
    "    \"What are the main symptoms of Type 2 diabetes?\",\n",
    "    \"What is the mechanism of action of metformin?\",\n",
    "    \"What are the risk factors for developing hypertension?\",\n",
    "]\n",
    "\n",
    "\n",
    "def generate_response(model, tokenizer, question, max_new_tokens=256):\n",
    "    \"\"\"Generate a response using the chat template.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "    # Decode only the generated tokens (skip the input)\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True\n",
    "    )\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASE MODEL RESPONSES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "base_responses = []\n",
    "for q in test_questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    response = generate_response(model, tokenizer, q)\n",
    "    base_responses.append(response)\n",
    "    print(f\"A: {response[:500]}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Dataset\n",
    "\n",
    "We'll use the **Medical Meadow Medical Flashcards** dataset — 33K medical Q&A pairs from HuggingFace Hub. For this tutorial, we only use 1,000 training examples and 200 for evaluation to keep training fast.\n",
    "\n",
    "**Data format:** SFTTrainer expects conversations in the chat message format that matches the model's chat template. Each example becomes:\n",
    "```\n",
    "[{\"role\": \"user\", \"content\": <question>}, {\"role\": \"assistant\", \"content\": <answer>}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading medical flashcards dataset...\n",
      "Total examples: 33,955\n",
      "\n",
      "Columns: ['input', 'output', 'instruction']\n",
      "\n",
      "Example:\n",
      "  input: What is the relationship between very low Mg2+ levels, PTH levels, and Ca2+ levels?\n",
      "  output: Very low Mg2+ levels correspond to low PTH levels which in turn results in low Ca2+ levels.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading medical flashcards dataset...\")\n",
    "raw_dataset = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\", split=\"train\")\n",
    "print(f\"Total examples: {len(raw_dataset):,}\")\n",
    "\n",
    "# Peek at the raw format\n",
    "print(f\"\\nColumns: {raw_dataset.column_names}\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  input: {raw_dataset[0]['input'][:200]}\")\n",
    "print(f\"  output: {raw_dataset[0]['output'][:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 1000\n",
      "Evaluation examples: 200\n",
      "\n",
      "Formatted example:\n",
      "[{'content': 'What type of injury to the arm/elbow most often leads to supracondylar fractures?', 'role': 'user'}, {'content': 'Supracondylar fractures most often occur after hyperextension injuries of the arm/elbow.', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "# Format into chat messages for SFTTrainer\n",
    "def format_to_chat(example):\n",
    "    \"\"\"Convert raw Q&A pair to chat message format.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example[\"input\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"output\"]},\n",
    "    ]\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "# Shuffle and select subsets\n",
    "shuffled = raw_dataset.shuffle(seed=42)\n",
    "train_dataset = shuffled.select(range(1000)).map(format_to_chat)\n",
    "eval_dataset = shuffled.select(range(1000, 1200)).map(format_to_chat)\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Evaluation examples: {len(eval_dataset)}\")\n",
    "print(f\"\\nFormatted example:\")\n",
    "print(train_dataset[0][\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure LoRA\n",
    "\n",
    "### What is LoRA?\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** is a technique that makes fine-tuning practical by freezing all original model weights and injecting small trainable matrices into specific layers.\n",
    "\n",
    "Instead of updating a weight matrix **W** (e.g., 2048×2048 = 4M params), LoRA decomposes the update into two small matrices: **A** (2048×8) and **B** (8×2048). That's only 32K params instead of 4M — a 125x reduction.\n",
    "\n",
    "The key hyperparameters:\n",
    "- **rank (`r`)**: Size of the low-rank matrices. Higher = more capacity, more memory. We use 8.\n",
    "- **alpha**: Scaling factor. Convention is 2× rank. We use 16.\n",
    "- **target modules**: Which layers get LoRA adapters. We target the attention projections (`q_proj`, `k_proj`, `v_proj`, `o_proj`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,490,944 || all params: 1,001,376,896 || trainable%: 0.1489\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                           # Rank of the low-rank matrices\n",
    "    lora_alpha=16,                 # Scaling factor (convention: 2x rank)\n",
    "    target_modules=[               # Which layers to add LoRA to\n",
    "        \"q_proj\",                  #   Query projection\n",
    "        \"k_proj\",                  #   Key projection\n",
    "        \"v_proj\",                  #   Value projection\n",
    "        \"o_proj\",                  #   Output projection\n",
    "    ],\n",
    "    lora_dropout=0.05,             # Small dropout for regularization\n",
    "    bias=\"none\",                   # Don't train bias terms\n",
    "    task_type=\"CAUSAL_LM\",         # We're doing causal language modeling\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Show the parameter breakdown\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact numbers depend on the model version, but the key point is the same — we're only training **~0.15%** of the parameters. That's the magic of LoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train\n",
    "\n",
    "We use `SFTTrainer` (Supervised Fine-Tuning Trainer) from the `trl` library. It handles chat template formatting, packing, and the training loop.\n",
    "\n",
    "**Training config rationale:**\n",
    "- **batch_size=1** with **gradient_accumulation=4**: Keeps peak memory low while simulating batch_size=4\n",
    "- **1 epoch**: ~250 steps, enough to see the model learn without overfitting\n",
    "- **max_seq_length=512**: Covers most flashcard Q&A pairs while keeping memory reasonable\n",
    "- **learning_rate=2e-4**: Standard for LoRA fine-tuning\n",
    "- **num_workers=0, pin_memory=False**: Required for MPS compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1000\n",
      "Eval samples: 200\n",
      "Effective batch size: 2\n",
      "Steps per epoch: ~500\n",
      "\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    # Output\n",
    "    output_dir=\"./output\",\n",
    "\n",
    "    # Training duration\n",
    "    num_train_epochs=1,\n",
    "    max_steps=-1,                    # -1 means use num_train_epochs\n",
    "\n",
    "    # Batch size & memory\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_length=512,\n",
    "\n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    # Optimizer\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=25,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    # Logging & eval\n",
    "    logging_steps=25,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "\n",
    "    # MPS compatibility\n",
    "    bf16=False,\n",
    "    fp16=False,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False,\n",
    "\n",
    "    # Misc\n",
    "    seed=42,\n",
    "    report_to=\"none\",                # No wandb/tensorboard for this tutorial\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Eval samples: {len(eval_dataset)}\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Steps per epoch: ~{len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")\n",
    "print(f\"\\nStarting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 51:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>5.182949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.049642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.928152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.739538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.721630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.604375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.670352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.578549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.579519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.522842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.473889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.446678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.336286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.371435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.366203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.390232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.366602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.348285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.396912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.391642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete!\n",
      "Total steps: 500\n",
      "Final training loss: 1.7733\n"
     ]
    }
   ],
   "source": [
    "# This is the actual training cell — takes ~15-20 min on M3 Pro\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Total steps: {train_result.global_step}\")\n",
    "print(f\"Final training loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the training loss decrease over the ~250 steps. A typical pattern:\n",
    "- Step 25: loss ~2.5-3.0\n",
    "- Step 100: loss ~1.5-2.0  \n",
    "- Step 250: loss ~1.0-1.5\n",
    "\n",
    "If your loss isn't decreasing, something went wrong with the data formatting or config."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate: Base vs Fine-Tuned\n",
    "\n",
    "Now let's ask the same 3 questions and compare the responses. The fine-tuned model should give more focused, medical-flashcard-style answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPARISON: BASE vs FINE-TUNED\n",
      "============================================================\n",
      "\n",
      "Q: What are the main symptoms of Type 2 diabetes?\n",
      "\n",
      "[BASE MODEL]:\n",
      "Okay, let's break down the main symptoms of Type 2 diabetes. It's really important to remember that many people with Type 2 diabetes have *no* noticeable symptoms at all, especially in the early stages. This is why regular check-ups are so crucial. However, when symptoms do appear, they can be varied and often mimic other conditions.\n",
      "\n",
      "Here's a breakdown of the common symptoms, grouped by severity:\n",
      "\n",
      "**1. Early & Subtle Symptoms (Often Overlooked):**\n",
      "\n",
      "* **Increased Thirst (Polydipsia):** Feeling v\n",
      "\n",
      "[FINE-TUNED]:\n",
      "Type 2 diabetes can be diagnosed by looking for symptoms like increased thirst, frequent urination, and fatigue. These symptoms can occur in individuals with Type 2 diabetes, as well as in those who have a family history of the condition. Type 2 diabetes is a chronic condition that is characterized by high blood sugar levels, which can lead to complications such as kidney disease, nerve damage, and heart disease. Early diagnosis and management of Type 2 diabetes can help to prevent or delay thes\n",
      "============================================================\n",
      "\n",
      "Q: What is the mechanism of action of metformin?\n",
      "\n",
      "[BASE MODEL]:\n",
      "Metformin is a cornerstone medication for type 2 diabetes, and its mechanism of action is complex and still being actively researched. It’s not a single \"magic bullet,\" but rather a multifaceted effect on glucose metabolism. Here's a breakdown of the current understanding:\n",
      "\n",
      "**1. Primarily Reduces Glucose Production in the Liver:**\n",
      "\n",
      "* **AMP-Activated Protein Kinase (AMPK) Activation:** This is the most well-established mechanism. Metformin primarily works by activating AMPK, a cellular energy sen\n",
      "\n",
      "[FINE-TUNED]:\n",
      "Metformin is an oral medication that is used to lower blood sugar levels in people with diabetes.\n",
      "============================================================\n",
      "\n",
      "Q: What are the risk factors for developing hypertension?\n",
      "\n",
      "[BASE MODEL]:\n",
      "Okay, let's break down the risk factors for developing hypertension (high blood pressure). It’s a complex issue with many contributing factors, and often it's a combination of several things. Here's a breakdown, categorized for clarity:\n",
      "\n",
      "**1. Lifestyle Factors (These are often the biggest contributors):**\n",
      "\n",
      "* **Diet:**\n",
      "    * **High sodium intake:** This is arguably the *most* significant risk factor.  Excess sodium causes your body to retain water, increasing blood volume and raising blood pressu\n",
      "\n",
      "[FINE-TUNED]:\n",
      "Hypertension is the most common cardiovascular disease, and the risk factors for developing hypertension are high blood pressure, age, family history of hypertension, and the presence of other cardiovascular risk factors such as diabetes and obesity.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Disable gradient checkpointing for inference — it conflicts with the KV cache\n",
    "# that generate() uses for autoregressive decoding\n",
    "model.gradient_checkpointing_disable()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON: BASE vs FINE-TUNED\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, q in enumerate(test_questions):\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"\\n[BASE MODEL]:\")\n",
    "    print(f\"{base_responses[i][:500]}\")\n",
    "    print(f\"\\n[FINE-TUNED]:\")\n",
    "    ft_response = generate_response(model, tokenizer, q)\n",
    "    print(f\"{ft_response[:500]}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save & Load Adapter\n",
    "\n",
    "One of the best things about LoRA: the adapter weights are **tiny**. The full model is ~4GB, but the LoRA adapter is just a few MB. You can save multiple fine-tuned versions without duplicating the base model.\n",
    "\n",
    "The pattern for deployment:\n",
    "1. Save adapter weights (few MB)\n",
    "2. Load base model\n",
    "3. Load adapter on top\n",
    "4. Optionally merge adapter into base model for faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter saved to: ./output/medical-flashcards-adapter\n",
      "Adapter size: 37.6 MB\n",
      "(vs ~4,000 MB for the full model)\n"
     ]
    }
   ],
   "source": [
    "# Save the LoRA adapter\n",
    "adapter_path = \"./output/medical-flashcards-adapter\"\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "\n",
    "# Show how small the adapter is\n",
    "import pathlib\n",
    "adapter_size = sum(\n",
    "    f.stat().st_size for f in pathlib.Path(adapter_path).rglob(\"*\") if f.is_file()\n",
    ")\n",
    "print(f\"Adapter saved to: {adapter_path}\")\n",
    "print(f\"Adapter size: {adapter_size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"(vs ~4,000 MB for the full model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fresh base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8485cf300b2e49e4b39a0af10da0df8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapter on top...\n",
      "\n",
      "Q: What are the main symptoms of Type 2 diabetes?\n",
      "A: Type 2 diabetes is characterized by high blood sugar levels, as well as the presence of high cholesterol levels.\n",
      "\n",
      "Adapter loaded and working!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate loading the adapter onto a fresh base model\n",
    "from peft import PeftModel\n",
    "\n",
    "print(\"Loading fresh base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"mps\",\n",
    ")\n",
    "\n",
    "print(\"Loading LoRA adapter on top...\")\n",
    "loaded_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# Verify it works\n",
    "response = generate_response(loaded_model, tokenizer, test_questions[0])\n",
    "print(f\"\\nQ: {test_questions[0]}\")\n",
    "print(f\"A: {response[:300]}\")\n",
    "print(\"\\nAdapter loaded and working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "You've completed a full LoRA fine-tuning loop! Here's how to scale up:\n",
    "\n",
    "### Immediate improvements\n",
    "- **More data**: Use all 33K flashcards instead of 1K\n",
    "- **Higher LoRA rank**: Try `r=16` or `r=32` for more capacity\n",
    "- **More target modules**: Add `gate_proj`, `up_proj`, `down_proj` (feed-forward layers)\n",
    "- **Multiple epochs**: 2-3 epochs with early stopping\n",
    "\n",
    "### Scaling to GPU (RunPod)\n",
    "- **QLoRA**: Use `bitsandbytes` to load the model in 4-bit, then apply LoRA. Needs CUDA GPU.\n",
    "- **Larger models**: Gemma 3 4B, 12B, or 27B with QLoRA on an A100/H100\n",
    "- **MedGemma**: Same architecture as Gemma — this exact notebook works with `google/medgemma-4b-it` or `google/medgemma-27b-it` on a GPU with enough VRAM\n",
    "\n",
    "### For entity resolution\n",
    "- Fine-tune MedGemma on your entity resolution training pairs\n",
    "- Format: patient record pairs → match/no-match with reasoning\n",
    "- The same LoRA approach works — just change the dataset and possibly the target modules\n",
    "\n",
    "### Key config changes for GPU\n",
    "```python\n",
    "# QLoRA on CUDA GPU\n",
    "from transformers import BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/medgemma-27b-it\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
