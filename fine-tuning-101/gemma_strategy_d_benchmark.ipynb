{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Gemma 1B Base Model Benchmark with Strategy D Summaries\n",
    "\n",
    "Benchmark the **unfine-tuned Gemma 3 1B** on entity resolution using Strategy D (Structured Diff-Friendly) summaries.\n",
    "\n",
    "**Why?** The summarizer optimization notebook showed Strategy D achieves F1=0.936 with Opus as the judge at ~760 tok/summary. Before fine-tuning Gemma 1B on this task, we need a baseline: how well does the base model perform?\n",
    "\n",
    "This establishes the \"before\" number that fine-tuning will improve upon. The existing `entity_resolution_fine_tuning.ipynb` only benchmarked base Gemma on condensed summaries (~220 tok), which scored poorly (Opus got F1=0.558 on those). Strategy D summaries are much richer and should give a stronger baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0\n",
      "MPS available: True\n",
      "Using device: mps\n",
      "Project root: /Users/alex/repos/Kaggle/SyntheticMass\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# Add project root so shared/ imports work\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Add llm-entity-resolution for summarizer imports\n",
    "_llm_er_root = os.path.join(PROJECT_ROOT, \"llm-entity-resolution\")\n",
    "if _llm_er_root not in sys.path:\n",
    "    sys.path.insert(0, _llm_er_root)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as: abicyclerider\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login, whoami\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "token = os.environ.get(\"HF_TOKEN\")\n",
    "if not token:\n",
    "    raise ValueError(\"HF_TOKEN not found. Create a .env file with: HF_TOKEN=hf_your_token_here\")\n",
    "\n",
    "login(token=token)\n",
    "print(f\"Logged in as: {whoami()['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model from google/gemma-3-1b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a09b2e485594031890bed09541fb60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on: mps:0\n",
      "Parameters: 999,885,952\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"google/gemma-3-1b-it\"\n",
    "\n",
    "print(f\"Loading tokenizer and model from {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    dtype=torch.float32,\n",
    "    device_map=\"mps\",\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on: {model.device}\")\n",
    "print(f\"Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Data Loading & Benchmark Pair Selection\n",
    "\n",
    "Reproduce the exact same 50 benchmark pairs (25 match + 25 non-match) from `summarizer_optimization.ipynb` using the same random seed and pair generation logic. This ensures results are directly comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True pairs: 1121, Non-match pairs: 1121\n",
      "Test pairs: 100 (50 match + 50 non-match)\n",
      "Benchmark pairs: 50 (25 match + 25 non-match)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shared.data_loader import load_facility_patients\n",
    "from shared.ground_truth import (\n",
    "    load_ground_truth,\n",
    "    add_record_ids_to_ground_truth,\n",
    "    generate_true_pairs_from_ground_truth,\n",
    ")\n",
    "from shared.medical_records import load_medical_records, get_patient_records\n",
    "\n",
    "RUN_DIR = os.path.join(PROJECT_ROOT, \"output\", \"augmented\", \"run_20260203_071928\")\n",
    "\n",
    "# Load patients and create record_ids\n",
    "patients_df = load_facility_patients(RUN_DIR)\n",
    "patients_df['record_id'] = patients_df['facility_id'] + '_' + patients_df['id'].astype(str)\n",
    "\n",
    "# Load ground truth and add record_ids\n",
    "ground_truth_df = load_ground_truth(RUN_DIR)\n",
    "ground_truth_df = add_record_ids_to_ground_truth(ground_truth_df, patients_df)\n",
    "\n",
    "# Generate true match pairs\n",
    "true_pairs = generate_true_pairs_from_ground_truth(ground_truth_df)\n",
    "\n",
    "# Build record_id -> (patient_uuid, facility_id) mapping\n",
    "record_map = {}\n",
    "for _, row in patients_df.iterrows():\n",
    "    record_map[row['record_id']] = (row['id'], row['facility_id'])\n",
    "\n",
    "# Load medical records\n",
    "medical_records = load_medical_records(RUN_DIR)\n",
    "\n",
    "# Generate non-match pairs (same code + seed as fine-tuning notebook)\n",
    "rid_to_true_id = (ground_truth_df.dropna(subset=['record_id'])\n",
    "                  .set_index('record_id')['true_patient_id'].to_dict())\n",
    "all_record_ids = list(rid_to_true_id.keys())\n",
    "\n",
    "random.seed(42)\n",
    "non_match_pairs = set()\n",
    "target = len(true_pairs)\n",
    "attempts = 0\n",
    "while len(non_match_pairs) < target and attempts < target * 20:\n",
    "    r1, r2 = random.sample(all_record_ids, 2)\n",
    "    if rid_to_true_id.get(r1) != rid_to_true_id.get(r2):\n",
    "        non_match_pairs.add(tuple(sorted([r1, r2])))\n",
    "    attempts += 1\n",
    "\n",
    "# Reproduce train/eval/test split (same as fine-tuning notebook)\n",
    "all_pairs = ([(r1, r2, True) for r1, r2 in true_pairs] +\n",
    "             [(r1, r2, False) for r1, r2 in non_match_pairs])\n",
    "random.shuffle(all_pairs)\n",
    "\n",
    "# Filter to pairs with records\n",
    "all_pairs = [(r1, r2, l) for r1, r2, l in all_pairs\n",
    "             if r1 in record_map and r2 in record_map]\n",
    "\n",
    "matches = [p for p in all_pairs if p[2]]\n",
    "non_matches = [p for p in all_pairs if not p[2]]\n",
    "\n",
    "n_total = min(len(matches), len(non_matches))\n",
    "n_train = min(500, int(n_total * 0.70))\n",
    "n_eval = min(100, int(n_total * 0.15))\n",
    "n_test = min(50, n_total - n_train - n_eval)\n",
    "\n",
    "test_pairs = (matches[n_train+n_eval:n_train+n_eval+n_test] +\n",
    "              non_matches[n_train+n_eval:n_train+n_eval+n_test])\n",
    "random.shuffle(test_pairs)\n",
    "\n",
    "# Same 50-pair selection as summarizer_optimization notebook\n",
    "benchmark_matches = [p for p in test_pairs if p[2]][:25]\n",
    "benchmark_non_matches = [p for p in test_pairs if not p[2]][:25]\n",
    "benchmark_pairs = benchmark_matches + benchmark_non_matches\n",
    "random.shuffle(benchmark_pairs)\n",
    "\n",
    "print(f\"True pairs: {len(true_pairs)}, Non-match pairs: {len(non_match_pairs)}\")\n",
    "print(f\"Test pairs: {len(test_pairs)} ({sum(1 for _,_,l in test_pairs if l)} match + \"\n",
    "      f\"{sum(1 for _,_,l in test_pairs if not l)} non-match)\")\n",
    "print(f\"Benchmark pairs: {len(benchmark_pairs)} \"\n",
    "      f\"({len(benchmark_matches)} match + {len(benchmark_non_matches)} non-match)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Strategy D: Structured Diff-Friendly Summarizer\n",
    "\n",
    "From `summarizer_optimization.ipynb` — organizes clinical data by category with year grouping:\n",
    "- **Conditions** grouped by onset year with ongoing status\n",
    "- **Medications** as drug (start–end or ongoing)\n",
    "- **Allergies** as flat list\n",
    "- **Key observations**: latest 2 values per metric\n",
    "- **Procedures** with years, sorted chronologically\n",
    "\n",
    "Achieves F1=0.936 with Opus at ~760 tok/summary (~1508 tok/pair)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Strategy D summaries for 95 unique records...\n",
      "\n",
      "Single summary token lengths:\n",
      "  Mean: 722, Median: 554, Max: 2857\n",
      "\n",
      "Pair token lengths (summaries only):\n",
      "  Mean: 1424, Median: 1325, Max: 3447\n",
      "\n",
      "============================================================\n",
      "Example summary (facility_001_2fc3ff4f-6a9b-b756-f92e-eb6f1f3a00a8):\n",
      "============================================================\n",
      "CONDITIONS:\n",
      "  2025: Medication review due (situation)\n",
      "\n",
      "OBSERVATIONS:\n",
      "- Body Height: 56.2 cm (2025-11-02)\n",
      "- Body Weight: 4.6 kg (2025-11-02)\n",
      "- Systolic Blood Pressure: 130.0 mm[Hg] (2025-11-02)\n",
      "- Diastolic Blood Pressure: 78.0 mm[Hg] (2025-11-02)\n",
      "\n",
      "PROCEDURES:\n",
      "- Medication Reconciliation (procedure) (2025)\n"
     ]
    }
   ],
   "source": [
    "def summarize_diff_friendly(patient_id, facility_id, medical_records):\n",
    "    \"\"\"Structured for pairwise comparison, grouped by year. Target ~800 tokens.\"\"\"\n",
    "    records = get_patient_records(patient_id, facility_id, medical_records)\n",
    "    sections = []\n",
    "\n",
    "    # CONDITIONS - grouped by onset year\n",
    "    cond_df = records.get('conditions')\n",
    "    if cond_df is not None:\n",
    "        cond_df = cond_df.copy()\n",
    "        cond_df['year'] = pd.to_datetime(cond_df['START'], errors='coerce').dt.year\n",
    "        cond_df['is_ongoing'] = cond_df['STOP'].isna() | (cond_df['STOP'].astype(str).str.strip() == '')\n",
    "        lines = [\"CONDITIONS:\"]\n",
    "        for year, grp in sorted(cond_df.groupby('year')):\n",
    "            if pd.isna(year):\n",
    "                continue\n",
    "            descs = []\n",
    "            for _, row in grp.iterrows():\n",
    "                status = \" *\" if row['is_ongoing'] else \"\"\n",
    "                descs.append(f\"{row['DESCRIPTION']}{status}\")\n",
    "            lines.append(f\"  {int(year)}: {'; '.join(descs)}\")\n",
    "        sections.append(\"\\n\".join(lines))\n",
    "\n",
    "    # MEDICATIONS - drug (start_year-end_year or ongoing)\n",
    "    meds_df = records.get('medications')\n",
    "    if meds_df is not None:\n",
    "        meds_df = meds_df.copy()\n",
    "        lines = [\"MEDICATIONS:\"]\n",
    "        for desc, grp in meds_df.groupby('DESCRIPTION', sort=False):\n",
    "            start_dt = pd.to_datetime(grp['START'], errors='coerce').min()\n",
    "            is_current = grp['STOP'].isna().any() | (grp['STOP'].astype(str).str.strip() == '').any()\n",
    "            if is_current:\n",
    "                period = f\"{start_dt.year}\\u2013ongoing\" if pd.notna(start_dt) else \"ongoing\"\n",
    "            else:\n",
    "                end_dt = pd.to_datetime(grp['STOP'], errors='coerce').max()\n",
    "                if pd.notna(start_dt) and pd.notna(end_dt):\n",
    "                    period = f\"{start_dt.year}\\u2013{end_dt.year}\"\n",
    "                else:\n",
    "                    period = \"unknown\"\n",
    "            lines.append(f\"- {desc} ({period})\")\n",
    "        sections.append(\"\\n\".join(lines))\n",
    "\n",
    "    # ALLERGIES - flat list\n",
    "    allg_df = records.get('allergies')\n",
    "    if allg_df is not None:\n",
    "        names = sorted(allg_df['DESCRIPTION'].unique())\n",
    "        sections.append(\"ALLERGIES: \" + \"; \".join(names))\n",
    "\n",
    "    # KEY OBSERVATIONS - latest 2 values per metric\n",
    "    obs_df = records.get('observations')\n",
    "    if obs_df is not None:\n",
    "        obs_df = obs_df.copy()\n",
    "        obs_df['date_dt'] = pd.to_datetime(obs_df['DATE'], errors='coerce')\n",
    "        key_obs = [\n",
    "            'Body Height', 'Body Weight', 'Body Mass Index',\n",
    "            'Systolic Blood Pressure', 'Diastolic Blood Pressure',\n",
    "            'Hemoglobin A1c/Hemoglobin.total in Blood',\n",
    "            'Glucose', 'Total Cholesterol',\n",
    "        ]\n",
    "        lines = [\"OBSERVATIONS:\"]\n",
    "        for obs_name in key_obs:\n",
    "            match = obs_df[obs_df['DESCRIPTION'].str.contains(obs_name, case=False, na=False)]\n",
    "            if match.empty:\n",
    "                continue\n",
    "            recent = match.sort_values('date_dt').tail(2)\n",
    "            vals = []\n",
    "            for _, row in recent.iterrows():\n",
    "                v = row.get('VALUE', '')\n",
    "                u = row.get('UNITS', '')\n",
    "                d = str(row.get('DATE', ''))[:10]\n",
    "                if pd.notna(v) and str(v).strip():\n",
    "                    u_str = f\" {u}\" if pd.notna(u) and u else \"\"\n",
    "                    vals.append(f\"{v}{u_str} ({d})\")\n",
    "            if vals:\n",
    "                lines.append(f\"- {obs_name}: {', '.join(vals)}\")\n",
    "        sections.append(\"\\n\".join(lines))\n",
    "\n",
    "    # PROCEDURES - with years, chronological\n",
    "    proc_df = records.get('procedures')\n",
    "    if proc_df is not None:\n",
    "        proc_df = proc_df.copy()\n",
    "        proc_df['year'] = pd.to_datetime(proc_df['START'], errors='coerce').dt.year\n",
    "        lines = [\"PROCEDURES:\"]\n",
    "        for desc, grp in proc_df.groupby('DESCRIPTION', sort=False):\n",
    "            years = sorted(grp['year'].dropna().unique())\n",
    "            year_strs = [str(int(y)) for y in years]\n",
    "            lines.append(f\"- {desc} ({', '.join(year_strs)})\")\n",
    "        sections.append(\"\\n\".join(lines))\n",
    "\n",
    "    return \"\\n\\n\".join(sections) if sections else \"No clinical records available.\"\n",
    "\n",
    "\n",
    "# Build summary cache for all unique record_ids in benchmark_pairs\n",
    "unique_rids = set()\n",
    "for r1, r2, _ in benchmark_pairs:\n",
    "    unique_rids.add(r1)\n",
    "    unique_rids.add(r2)\n",
    "\n",
    "print(f\"Generating Strategy D summaries for {len(unique_rids)} unique records...\")\n",
    "summary_cache = {}\n",
    "for rid in sorted(unique_rids):\n",
    "    if rid in record_map:\n",
    "        pid, fid = record_map[rid]\n",
    "        summary_cache[rid] = summarize_diff_friendly(pid, fid, medical_records)\n",
    "\n",
    "# Token length stats\n",
    "token_lengths = [len(tokenizer.encode(s)) for s in summary_cache.values()]\n",
    "pair_lengths = []\n",
    "for r1, r2, _ in benchmark_pairs:\n",
    "    if r1 in summary_cache and r2 in summary_cache:\n",
    "        combined = summary_cache[r1] + \"\\n\\n\" + summary_cache[r2]\n",
    "        pair_lengths.append(len(tokenizer.encode(combined)))\n",
    "\n",
    "print(f\"\\nSingle summary token lengths:\")\n",
    "print(f\"  Mean: {np.mean(token_lengths):.0f}, Median: {np.median(token_lengths):.0f}, Max: {max(token_lengths)}\")\n",
    "print(f\"\\nPair token lengths (summaries only):\")\n",
    "print(f\"  Mean: {np.mean(pair_lengths):.0f}, Median: {np.median(pair_lengths):.0f}, Max: {max(pair_lengths)}\")\n",
    "\n",
    "# Show one example\n",
    "example_rid = sorted(summary_cache.keys())[0]\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Example summary ({example_rid}):\")\n",
    "print(f\"{'='*60}\")\n",
    "print(summary_cache[example_rid][:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Benchmark: Base Gemma 1B on Strategy D Summaries\n",
    "\n",
    "Run the unfine-tuned model on all 50 benchmark pairs with greedy decoding. `max_length=4096` to accommodate Strategy D pairs (up to ~3400 tokens with chat template overhead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking base Gemma 1B on 50 pairs (Strategy D summaries)...\n",
      "  (25 match + 25 non-match)\n",
      "\n",
      "  10/50... (5/10 correct)\n",
      "  20/50... (9/20 correct)\n",
      "  30/50... (16/30 correct)\n",
      "  40/50... (20/40 correct)\n",
      "  50/50... (24/50 correct)\n",
      "\n",
      "Base Gemma 1B + Strategy D Results (50 parseable / 0 unparseable / 50 total):\n",
      "    accuracy: 0.480\n",
      "   precision: 0.486\n",
      "      recall: 0.680\n",
      "          f1: 0.567\n",
      "\n",
      "Confusion Matrix (rows=actual, cols=predicted):\n",
      "[[ 7 18]\n",
      " [ 8 17]]\n",
      "\n",
      "Prediction distribution: 35 True / 15 False\n",
      "Actual distribution:     25 True / 25 False\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "INSTRUCTION = (\n",
    "    \"You are a medical record matching expert. Compare these two patient \"\n",
    "    \"medical records and determine if they belong to the same patient based \"\n",
    "    \"only on their clinical history.\\n\\n\"\n",
    "    \"Record A:\\n{summary_a}\\n\\n\"\n",
    "    \"Record B:\\n{summary_b}\\n\\n\"\n",
    "    \"Are these the same patient? Answer only 'True' or 'False'.\"\n",
    ")\n",
    "\n",
    "\n",
    "def predict_match(model, tokenizer, summary_a, summary_b):\n",
    "    \"\"\"Predict whether two medical records are from the same patient.\"\"\"\n",
    "    prompt = INSTRUCTION.format(summary_a=summary_a, summary_b=summary_b)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(\n",
    "        input_text, return_tensors=\"pt\", truncation=True, max_length=4096\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=8, do_sample=False)\n",
    "\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True\n",
    "    ).strip().lower()\n",
    "\n",
    "    if \"true\" in response:\n",
    "        return True\n",
    "    elif \"false\" in response:\n",
    "        return False\n",
    "    return None\n",
    "\n",
    "\n",
    "print(f\"Benchmarking base Gemma 1B on {len(benchmark_pairs)} pairs (Strategy D summaries)...\")\n",
    "print(f\"  ({sum(1 for _,_,l in benchmark_pairs if l)} match + \"\n",
    "      f\"{sum(1 for _,_,l in benchmark_pairs if not l)} non-match)\\n\")\n",
    "\n",
    "preds, labels, unparseable = [], [], 0\n",
    "for i, (r1, r2, label) in enumerate(benchmark_pairs):\n",
    "    pred = predict_match(model, tokenizer, summary_cache[r1], summary_cache[r2])\n",
    "    if pred is not None:\n",
    "        preds.append(pred)\n",
    "        labels.append(label)\n",
    "    else:\n",
    "        unparseable += 1\n",
    "    if (i + 1) % 10 == 0:\n",
    "        correct = sum(p == l for p, l in zip(preds, labels))\n",
    "        print(f\"  {i+1}/{len(benchmark_pairs)}... ({correct}/{len(preds)} correct)\")\n",
    "\n",
    "# Metrics\n",
    "metrics = {\n",
    "    'accuracy': accuracy_score(labels, preds),\n",
    "    'precision': precision_score(labels, preds, zero_division=0),\n",
    "    'recall': recall_score(labels, preds, zero_division=0),\n",
    "    'f1': f1_score(labels, preds, zero_division=0),\n",
    "}\n",
    "\n",
    "print(f\"\\nBase Gemma 1B + Strategy D Results ({len(preds)} parseable / \"\n",
    "      f\"{unparseable} unparseable / {len(benchmark_pairs)} total):\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric:>10s}: {value:.3f}\")\n",
    "print(f\"\\nConfusion Matrix (rows=actual, cols=predicted):\")\n",
    "print(confusion_matrix(labels, preds))\n",
    "\n",
    "# Prediction distribution\n",
    "print(f\"\\nPrediction distribution: {sum(preds)} True / {len(preds) - sum(preds)} False\")\n",
    "print(f\"Actual distribution:     {sum(labels)} True / {len(labels) - sum(labels)} False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Results & Context\n",
    "\n",
    "### Comparison with other configurations\n",
    "\n",
    "| Model | Summary Format | Tok/Summary | F1 |\n",
    "|-------|---------------|-------------|----|\n",
    "| Opus | Condensed (~220 tok) | ~200 | 0.558 |\n",
    "| Opus | Full summary (~1000 tok) | ~1019 | 0.889 |\n",
    "| Opus | Strategy D (~760 tok) | ~760 | **0.936** |\n",
    "| Opus | Raw records (~6000 tok) | ~6069 | 0.980 |\n",
    "| **Base Gemma 1B** | **Strategy D (~760 tok)** | **~760** | **see above** |\n",
    "\n",
    "This is the pre-fine-tuning baseline for Gemma 1B with Strategy D summaries. Fine-tuning should close the gap between this number and the Opus F1=0.936."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
