# Promptfoo configuration for medical record entity resolution testing
# Run: cd config/promptfoo && promptfoo eval -j 1 && promptfoo view

description: "Medical Record Entity Resolution â€” LLM Comparison"

prompts:
  - file://prompts/baseline_v1.txt
  - file://prompts/optimized.txt

providers:
  # MedGemma Q4-fast (small, ~39 tok/s)
  - id: openai:chat:medgemma:1.5-4b-q4-fast
    config:
      apiBaseUrl: http://localhost:11434/v1
      apiKey: ollama
      temperature: 0.1
      max_tokens: 256

  # MedGemma full 4B (larger, ~15 tok/s)
  - id: openai:chat:medgemma:1.5-4b
    config:
      apiBaseUrl: http://localhost:11434/v1
      apiKey: ollama
      temperature: 0.1
      max_tokens: 256

  # DSPy optimized module (after running optimize.py)
  - id: python:providers/dspy_provider.py
    label: "DSPy Optimized"
    config:
      model: "medgemma:1.5-4b-q4-fast"

tests: file://datasets/gray_zone_tests.yaml

# Run sequentially since Ollama serves one request at a time
defaultTest:
  options:
    provider:
      delay: 500  # ms between requests

outputPath: results.json
